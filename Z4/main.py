import pandas as pd
import numpy as np
import tensorflow as tf
import keras_tuner as kt
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, Input, Normalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import argparse
import sys
import os
import pickle
import matplotlib.pyplot as plt

DATA_FILE = "wine.data"
COLUMN_NAMES = [
    'Class', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash', 'Magnesium',
    'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins',
    'Color_intensity', 'Hue', 'OD280_OD315_ratio', 'Proline'
]
NORMALIZER_LAYER = None

def load_data(filepath):

    if not os.path.exists(filepath):
        print(f"Błąd: Nie znaleziono pliku {filepath}")
        return None

    data = pd.read_csv(filepath, header=None, names=COLUMN_NAMES)
    data = data.sample(frac=1, random_state=42).reset_index(drop=True)

    X = data.drop('Class', axis=1)
    y = data['Class'] - 1

    y_one_hot = to_categorical(y, num_classes=3)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_one_hot, test_size=0.2, random_state=42
    )

    return X_train, X_test, y_train, y_test


def create_model(normalization_layer, units, dropout_rate, learning_rate):

    model = Sequential()
    model.add(Input(shape=(13,)))
    model.add(normalization_layer)

    model.add(Dense(units, activation='relu', kernel_initializer='he_uniform'))

    if dropout_rate > 0:
        model.add(Dropout(dropout_rate))

    model.add(Dense(max(8, units // 2), activation='relu'))

    model.add(Dense(3, activation='softmax'))

    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model


def build_model_for_tuner(hp):

    hp_units = hp.Int('units', min_value=32, max_value=128, step=16)
    hp_dropout = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)
    hp_lr = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
    return create_model(NORMALIZER_LAYER, hp_units, hp_dropout, hp_lr)

def run_analysis():
    global NORMALIZER_LAYER

    data = load_data(DATA_FILE)
    if data is None: return
    X_train, X_test, y_train, y_test = data

    NORMALIZER_LAYER = Normalization(axis=-1)
    NORMALIZER_LAYER.adapt(np.array(X_train))

    baseline_model = load_model('wine_model.keras')
    baseline_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=16, verbose=1)
    baseline_score = baseline_model.evaluate(X_test, y_test, verbose=0)
    print(f"BASELINE Accuracy: {baseline_score[1]:.4f} (Loss: {baseline_score[0]:.4f})")

    tuner = kt.RandomSearch(
        build_model_for_tuner,
        objective='val_accuracy',
        max_trials=5,
        executions_per_trial=1,
        directory='my_tuner_dir',
        project_name='wine_optimization',
        overwrite=True
    )
    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
    tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[stop_early], verbose=1)
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

    print("\n--- Znaleziono najlepsze parametry ---")
    print(f"Liczba neuronów: {best_hps.get('units')}")
    print(f"Dropout: {best_hps.get('dropout')}")
    print(f"Learning Rate: {best_hps.get('learning_rate')}")
    best_model = tuner.hypermodel.build(best_hps)

    history = best_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=0)

    final_score = best_model.evaluate(X_test, y_test, verbose=0)
    print(f"\nBaseLine Model Accuracy: {baseline_score[1]:.4f}")
    print(f"\nTUNED Model Accuracy: {final_score[1]:.4f}")

    if final_score[1] >= baseline_score[1]:
        print(f"SUKCES: Poprawiono wynik baseline (+{final_score[1] - baseline_score[1]:.4f})")
    else:
        return

    best_model.save('wine_model_tuned.keras')
    print("Zapisano model do 'wine_model_tuned.keras'")

def predict_from_args():
    try:
        model = load_model('wine_model.keras')
        with open('scaler.pkl', 'rb') as f:
            scaler = pickle.load(f)
    except Exception as e:
        print(f"Błąd ładowania modelu lub scalera: {e}")
        return

    parser = argparse.ArgumentParser(description="Przewiduje klasę wina na podstawie 13 cech.")
    parser.add_argument('--alcohol', type=float, required=True)
    parser.add_argument('--malic_acid', type=float, required=True)
    parser.add_argument('--ash', type=float, required=True)
    parser.add_argument('--alcalinity', type=float, required=True)
    parser.add_argument('--magnesium', type=float, required=True)
    parser.add_argument('--phenols', type=float, required=True)
    parser.add_argument('--flavanoids', type=float, required=True)
    parser.add_argument('--nonflavanoid', type=float, required=True)
    parser.add_argument('--proanthocyanins', type=float, required=True)
    parser.add_argument('--color', type=float, required=True)
    parser.add_argument('--hue', type=float, required=True)
    parser.add_argument('--od_ratio', type=float, required=True)
    parser.add_argument('--proline', type=float, required=True)

    args = parser.parse_args(sys.argv[1:])

    input_data = np.array([[
        args.alcohol, args.malic_acid, args.ash, args.alcalinity,
        args.magnesium, args.phenols, args.flavanoids, args.nonflavanoid,
        args.proanthocyanins, args.color, args.hue, args.od_ratio, args.proline
    ]])

    input_data_scaled = scaler.transform(input_data)
    prediction = model.predict(input_data_scaled)
    predicted_index = np.argmax(prediction, axis=1)[0]
    predicted_class = predicted_index + 1

    print(f"Przewidywana kategoria wina: {predicted_class}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        predict_from_args()
        pass
    else:
        run_analysis()
